{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"E:/dataFiles/github/MFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 13) (891,)\n",
      "[ 0.    0.    1.    0.    1.   22.    1.    0.    7.25  0.    0.    0.\n",
      "  1.  ] -1\n"
     ]
    }
   ],
   "source": [
    "# 数据生成\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# TODO: 不使用pandas以及sklearn\n",
    "\n",
    "# 数据读取，并去掉第一列\n",
    "data = pd.read_csv(\"E:/dataFiles/github/MFlow/data/Titanic.csv\").drop(\n",
    "    [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "# 将字符串标签替换为数字并转换为one-hot\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "pclass = ohe.fit_transform(le.fit_transform(data[\"Pclass\"].fillna(0)).reshape(-1, 1))\n",
    "sex = ohe.fit_transform(le.fit_transform(data[\"Sex\"].fillna(\"\")).reshape(-1, 1))\n",
    "embarked = ohe.fit_transform(le.fit_transform(data[\"Embarked\"].fillna(\"\")).reshape(-1, 1))\n",
    "# 特征组合\n",
    "xs = np.concatenate([\n",
    "    pclass, sex, data[[\"Age\"]].fillna(0), data[[\"SibSp\"]].fillna(0),\n",
    "    data[[\"Parch\"]].fillna(0), data[[\"Fare\"]].fillna(0), embarked], axis=1)\n",
    "# 标签\n",
    "ys = data[\"Survived\"].values * 2 - 1\n",
    "\n",
    "# 特征维数\n",
    "FEAT_DIM = xs.shape[1]\n",
    "# 嵌入向量维度\n",
    "HIDDEN_DIM = 2\n",
    "\n",
    "print(xs.shape, ys.shape)\n",
    "print(xs[0], ys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, acc: 0.746.\n",
      "Epoch: 2, acc: 0.788.\n",
      "Epoch: 3, acc: 0.808.\n",
      "Epoch: 4, acc: 0.806.\n",
      "Epoch: 5, acc: 0.804.\n",
      "Epoch: 6, acc: 0.804.\n",
      "Epoch: 7, acc: 0.802.\n",
      "Epoch: 8, acc: 0.805.\n",
      "Epoch: 9, acc: 0.804.\n",
      "Epoch: 10, acc: 0.804.\n",
      "Epoch: 11, acc: 0.805.\n",
      "Epoch: 12, acc: 0.805.\n",
      "Epoch: 13, acc: 0.804.\n",
      "Epoch: 14, acc: 0.804.\n",
      "Epoch: 15, acc: 0.802.\n",
      "Epoch: 16, acc: 0.802.\n",
      "Epoch: 17, acc: 0.802.\n",
      "Epoch: 18, acc: 0.804.\n",
      "Epoch: 19, acc: 0.804.\n",
      "Epoch: 20, acc: 0.801.\n",
      "Epoch: 21, acc: 0.802.\n",
      "Epoch: 22, acc: 0.804.\n",
      "Epoch: 23, acc: 0.804.\n",
      "Epoch: 24, acc: 0.804.\n",
      "Epoch: 25, acc: 0.802.\n",
      "Epoch: 26, acc: 0.801.\n",
      "Epoch: 27, acc: 0.802.\n",
      "Epoch: 28, acc: 0.804.\n",
      "Epoch: 29, acc: 0.805.\n",
      "Epoch: 30, acc: 0.804.\n",
      "Epoch: 31, acc: 0.801.\n",
      "Epoch: 32, acc: 0.802.\n",
      "Epoch: 33, acc: 0.800.\n",
      "Epoch: 34, acc: 0.801.\n",
      "Epoch: 35, acc: 0.802.\n",
      "Epoch: 36, acc: 0.802.\n",
      "Epoch: 37, acc: 0.801.\n",
      "Epoch: 38, acc: 0.801.\n",
      "Epoch: 39, acc: 0.801.\n",
      "Epoch: 40, acc: 0.801.\n",
      "Epoch: 41, acc: 0.800.\n",
      "Epoch: 42, acc: 0.801.\n",
      "Epoch: 43, acc: 0.800.\n",
      "Epoch: 44, acc: 0.801.\n",
      "Epoch: 45, acc: 0.800.\n",
      "Epoch: 46, acc: 0.799.\n",
      "Epoch: 47, acc: 0.801.\n",
      "Epoch: 48, acc: 0.798.\n",
      "Epoch: 49, acc: 0.799.\n",
      "Epoch: 50, acc: 0.797.\n",
      "Epoch: 51, acc: 0.800.\n",
      "Epoch: 52, acc: 0.797.\n",
      "Epoch: 53, acc: 0.798.\n",
      "Epoch: 54, acc: 0.798.\n",
      "Epoch: 55, acc: 0.797.\n",
      "Epoch: 56, acc: 0.797.\n",
      "Epoch: 57, acc: 0.797.\n",
      "Epoch: 58, acc: 0.797.\n",
      "Epoch: 59, acc: 0.797.\n",
      "Epoch: 60, acc: 0.797.\n",
      "Epoch: 61, acc: 0.797.\n",
      "Epoch: 62, acc: 0.797.\n",
      "Epoch: 63, acc: 0.796.\n",
      "Epoch: 64, acc: 0.796.\n",
      "Epoch: 65, acc: 0.797.\n",
      "Epoch: 66, acc: 0.797.\n",
      "Epoch: 67, acc: 0.796.\n",
      "Epoch: 68, acc: 0.797.\n",
      "Epoch: 69, acc: 0.797.\n",
      "Epoch: 70, acc: 0.795.\n",
      "Epoch: 71, acc: 0.796.\n",
      "Epoch: 72, acc: 0.796.\n",
      "Epoch: 73, acc: 0.795.\n",
      "Epoch: 74, acc: 0.797.\n",
      "Epoch: 75, acc: 0.797.\n",
      "Epoch: 76, acc: 0.796.\n",
      "Epoch: 77, acc: 0.795.\n",
      "Epoch: 78, acc: 0.797.\n",
      "Epoch: 79, acc: 0.797.\n",
      "Epoch: 80, acc: 0.796.\n",
      "Epoch: 81, acc: 0.793.\n",
      "Epoch: 82, acc: 0.796.\n",
      "Epoch: 83, acc: 0.793.\n",
      "Epoch: 84, acc: 0.795.\n",
      "Epoch: 85, acc: 0.793.\n",
      "Epoch: 86, acc: 0.797.\n",
      "Epoch: 87, acc: 0.793.\n",
      "Epoch: 88, acc: 0.793.\n",
      "Epoch: 89, acc: 0.796.\n",
      "Epoch: 90, acc: 0.793.\n",
      "Epoch: 91, acc: 0.793.\n",
      "Epoch: 92, acc: 0.797.\n",
      "Epoch: 93, acc: 0.793.\n",
      "Epoch: 94, acc: 0.793.\n",
      "Epoch: 95, acc: 0.796.\n",
      "Epoch: 96, acc: 0.793.\n",
      "Epoch: 97, acc: 0.793.\n",
      "Epoch: 98, acc: 0.793.\n",
      "Epoch: 99, acc: 0.793.\n",
      "Epoch: 100, acc: 0.793.\n",
      "Epoch: 101, acc: 0.793.\n",
      "Epoch: 102, acc: 0.793.\n",
      "Epoch: 103, acc: 0.795.\n",
      "Epoch: 104, acc: 0.793.\n",
      "Epoch: 105, acc: 0.793.\n",
      "Epoch: 106, acc: 0.793.\n",
      "Epoch: 107, acc: 0.793.\n",
      "Epoch: 108, acc: 0.793.\n",
      "Epoch: 109, acc: 0.793.\n",
      "Epoch: 110, acc: 0.793.\n",
      "Epoch: 111, acc: 0.793.\n",
      "Epoch: 112, acc: 0.793.\n",
      "Epoch: 113, acc: 0.793.\n",
      "Epoch: 114, acc: 0.793.\n",
      "Epoch: 115, acc: 0.793.\n",
      "Epoch: 116, acc: 0.793.\n",
      "Epoch: 117, acc: 0.793.\n",
      "Epoch: 118, acc: 0.793.\n",
      "Epoch: 119, acc: 0.793.\n",
      "Epoch: 120, acc: 0.793.\n",
      "Epoch: 121, acc: 0.793.\n",
      "Epoch: 122, acc: 0.793.\n",
      "Epoch: 123, acc: 0.793.\n",
      "Epoch: 124, acc: 0.793.\n",
      "Epoch: 125, acc: 0.793.\n",
      "Epoch: 126, acc: 0.793.\n",
      "Epoch: 127, acc: 0.793.\n",
      "Epoch: 128, acc: 0.793.\n",
      "Epoch: 129, acc: 0.793.\n",
      "Epoch: 130, acc: 0.793.\n",
      "Epoch: 131, acc: 0.793.\n",
      "Epoch: 132, acc: 0.793.\n",
      "Epoch: 133, acc: 0.793.\n",
      "Epoch: 134, acc: 0.793.\n",
      "Epoch: 135, acc: 0.793.\n",
      "Epoch: 136, acc: 0.793.\n",
      "Epoch: 137, acc: 0.793.\n",
      "Epoch: 138, acc: 0.793.\n",
      "Epoch: 139, acc: 0.793.\n",
      "Epoch: 140, acc: 0.793.\n",
      "Epoch: 141, acc: 0.793.\n",
      "Epoch: 142, acc: 0.793.\n",
      "Epoch: 143, acc: 0.793.\n",
      "Epoch: 144, acc: 0.793.\n",
      "Epoch: 145, acc: 0.793.\n",
      "Epoch: 146, acc: 0.793.\n",
      "Epoch: 147, acc: 0.793.\n",
      "Epoch: 148, acc: 0.793.\n",
      "Epoch: 149, acc: 0.793.\n",
      "Epoch: 150, acc: 0.793.\n",
      "Epoch: 151, acc: 0.793.\n",
      "Epoch: 152, acc: 0.793.\n",
      "Epoch: 153, acc: 0.793.\n",
      "Epoch: 154, acc: 0.793.\n",
      "Epoch: 155, acc: 0.793.\n",
      "Epoch: 156, acc: 0.793.\n",
      "Epoch: 157, acc: 0.793.\n",
      "Epoch: 158, acc: 0.793.\n",
      "Epoch: 159, acc: 0.793.\n",
      "Epoch: 160, acc: 0.793.\n",
      "Epoch: 161, acc: 0.793.\n",
      "Epoch: 162, acc: 0.793.\n",
      "Epoch: 163, acc: 0.793.\n",
      "Epoch: 164, acc: 0.793.\n",
      "Epoch: 165, acc: 0.793.\n",
      "Epoch: 166, acc: 0.793.\n",
      "Epoch: 167, acc: 0.793.\n",
      "Epoch: 168, acc: 0.793.\n",
      "Epoch: 169, acc: 0.793.\n",
      "Epoch: 170, acc: 0.793.\n",
      "Epoch: 171, acc: 0.793.\n",
      "Epoch: 172, acc: 0.793.\n",
      "Epoch: 173, acc: 0.793.\n",
      "Epoch: 174, acc: 0.793.\n",
      "Epoch: 175, acc: 0.793.\n",
      "Epoch: 176, acc: 0.793.\n",
      "Epoch: 177, acc: 0.793.\n",
      "Epoch: 178, acc: 0.793.\n",
      "Epoch: 179, acc: 0.793.\n",
      "Epoch: 180, acc: 0.793.\n",
      "Epoch: 181, acc: 0.793.\n",
      "Epoch: 182, acc: 0.793.\n",
      "Epoch: 183, acc: 0.793.\n",
      "Epoch: 184, acc: 0.793.\n",
      "Epoch: 185, acc: 0.793.\n",
      "Epoch: 186, acc: 0.793.\n",
      "Epoch: 187, acc: 0.793.\n",
      "Epoch: 188, acc: 0.793.\n",
      "Epoch: 189, acc: 0.793.\n",
      "Epoch: 190, acc: 0.793.\n",
      "Epoch: 191, acc: 0.793.\n",
      "Epoch: 192, acc: 0.793.\n",
      "Epoch: 193, acc: 0.793.\n",
      "Epoch: 194, acc: 0.793.\n",
      "Epoch: 195, acc: 0.793.\n",
      "Epoch: 196, acc: 0.793.\n",
      "Epoch: 197, acc: 0.793.\n",
      "Epoch: 198, acc: 0.793.\n",
      "Epoch: 199, acc: 0.793.\n",
      "Epoch: 200, acc: 0.793.\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "from mflow import core, ops, opts, lays\n",
    "\n",
    "# 超参数\n",
    "lr = 0.005\n",
    "epoch = 200\n",
    "batch_size = 16\n",
    "\n",
    "with core.NameScope(\"DeepFM\"):\n",
    "    # 初始化变量\n",
    "    x = core.Variable(size=(FEAT_DIM, 1), trainable=False)\n",
    "    w = core.Variable(size=(1, FEAT_DIM), trainable=True)\n",
    "    x_pclass = core.Variable(size=(pclass.shape[1], 1), trainable=False)\n",
    "    x_sex = core.Variable(size=(sex.shape[1], 1), trainable=False)\n",
    "    x_embarked = core.Variable(size=(embarked.shape[1], 1), trainable=False)\n",
    "    w_pclass = core.Variable(size=(HIDDEN_DIM, pclass.shape[1]), trainable=True)\n",
    "    w_sex = core.Variable(size=(HIDDEN_DIM, sex.shape[1]), trainable=True)\n",
    "    w_embarked = core.Variable(size=(HIDDEN_DIM, embarked.shape[1]), trainable=True)\n",
    "    b = core.Variable(size=(1, 1), trainable=True)\n",
    "    y = core.Variable(size=(1, 1), trainable=False)\n",
    "    # 模型定义\n",
    "    # deep部分\n",
    "    embedding_pclass = ops.MatMul(w_pclass, x_pclass)\n",
    "    embedding_sex = ops.MatMul(w_sex, x_sex)\n",
    "    embedding_embarked = ops.MatMul(w_embarked, x_embarked)\n",
    "    embedding = ops.Concat(embedding_pclass, embedding_sex, embedding_embarked)\n",
    "    hidden_1 = lays.Linear(embedding, 3 * HIDDEN_DIM, 8, \"RELU\")\n",
    "    hidden_2 = lays.Linear(hidden_1, 8, 4, \"ReLU\")\n",
    "    deep = lays.Linear(hidden_2, 4, 1, None)\n",
    "    # FM部分\n",
    "    fm = ops.Add(\n",
    "        ops.MatMul(ops.Reshape(embedding, shape=(1, 3 * HIDDEN_DIM)), embedding),\n",
    "        ops.MatMul(w, x)\n",
    "    )\n",
    "    # 组合\n",
    "    pred = ops.Add(fm, deep, b)\n",
    "    predicter = ops.Logistic(pred)\n",
    "    loss = ops.loss.LogLoss(ops.Multiply(y, pred))\n",
    "    adam = opts.Adam(core.DefaultGraph, loss, lr)\n",
    "    # 开始训练\n",
    "    for ep in range(1, epoch + 1):\n",
    "        bs_idx = 0  # 批次计数\n",
    "        # 这是一个epoch的过程\n",
    "        for i, (feat, lab) in enumerate(zip(xs, ys)):\n",
    "            x.setValue(np.mat(feat).T)\n",
    "            x_pclass.setValue(np.mat(feat[:3]).T)\n",
    "            x_sex.setValue(np.mat(feat[3:5]).T)\n",
    "            x_embarked.setValue(np.mat(feat[9:]).T)\n",
    "            y.setValue(np.mat(lab))\n",
    "            adam.step()\n",
    "            bs_idx += 1\n",
    "            if bs_idx == batch_size:\n",
    "                adam.update()\n",
    "                bs_idx = 0\n",
    "        # 一个epoch完成后进行评估\n",
    "        preds = []\n",
    "        for feat in xs:\n",
    "            x.setValue(np.mat(feat).T)\n",
    "            x_pclass.setValue(np.mat(feat[:3]).T)\n",
    "            x_sex.setValue(np.mat(feat[3:5]).T)\n",
    "            x_embarked.setValue(np.mat(feat[9:]).T)\n",
    "            predicter.forward()\n",
    "            preds.append(predicter.value[0, 0])  # 结果\n",
    "        preds = (np.array(preds) > 0.5).astype(\"int\") * 2 - 1\n",
    "        acc = (ys == preds).astype(\"uint8\").sum() / len(xs)\n",
    "        print(\"Epoch: {:d}, acc: {:.3f}.\".format(ep, acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e464fba058b7062b0b3c4c12c3de33306c3fff5b5acf1746fee001474633a95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('iann': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
